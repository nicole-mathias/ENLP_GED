{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training the t5 base model with jleg data.**\n",
        "<p>This program is created to train the model with jfleg (https://huggingface.co/datasets/jfleg) and save the trained model.\n",
        "\n",
        "<p>The created pre-trained model will be used to test the shared tasted for English language (https://github.com/spraakbanken/multiged-2023)"
      ],
      "metadata": {
        "id": "RoQ8VDet83MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happytransformer\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kBFqMyCADmMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import HappyTextToText\n",
        "\n",
        "happy_tt = HappyTextToText(\"T5\", \"t5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHSQjQmtDntW",
        "outputId": "7f382f72-d404-4074-da18-754ceab0b6f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "1p2HpTjgDrnW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
        "\n",
        "eval_dataset = load_dataset(\"jfleg\", split='test[:]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIoqIPXVDt0p",
        "outputId": "93d37b15-4873-4188-a2a2-f4f1ad03152d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset jfleg (/root/.cache/huggingface/datasets/jfleg/default/1.0.0/ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b)\n",
            "WARNING:datasets.builder:Found cached dataset jfleg (/root/.cache/huggingface/datasets/jfleg/default/1.0.0/ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing the data**"
      ],
      "metadata": {
        "id": "fEJNsZJCcL0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def generate_csv(csv_path, dataset):\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        writter = csv.writer(csvfile)\n",
        "        writter.writerow([\"input\", \"target\"])\n",
        "        for case in dataset:\n",
        "     \t    # Adding the task's prefix to input \n",
        "            input_text = \"grammar: \" + case[\"sentence\"]\n",
        "            for correction in case[\"corrections\"]:\n",
        "                # a few of the cases contain blank strings. \n",
        "                if input_text and correction:\n",
        "                    writter.writerow([input_text, correction])\n",
        "                    \n",
        "\n",
        "\n",
        "generate_csv(\"train.csv\", train_dataset)\n",
        "generate_csv(\"eval.csv\", eval_dataset)"
      ],
      "metadata": {
        "id": "-7XuFGNJDvXY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_result = happy_tt.eval(\"eval.csv\")"
      ],
      "metadata": {
        "id": "0GqwCU6yDx8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**If program gives errors try running this part and then comment this part and restart the runtime**\n",
        "<p>!pip uninstall -y transformers accelerate\n",
        "<p>!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "XwxGjbBPL3yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If program gives errors try running this part and then comment this part and restart the runtime\n",
        "# !pip uninstall -y transformers accelerate\n",
        "# !pip install transformers accelerate"
      ],
      "metadata": {
        "id": "mfQ-fX2iErqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before loss:\", before_result.loss)"
      ],
      "metadata": {
        "id": "gVJrIHIKD0k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "h6axOmNPD6P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import TTTrainArgs\n",
        "\n",
        "args = TTTrainArgs(batch_size=8)\n",
        "happy_tt.train(\"train.csv\", args=args)"
      ],
      "metadata": {
        "id": "cYpBIHcCD-Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_loss = happy_tt.eval(\"eval.csv\")\n",
        "\n",
        "print(\"After loss: \", before_loss.loss)"
      ],
      "metadata": {
        "id": "H-JfUfu9D_Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import TTSettings\n",
        "\n",
        "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=50)"
      ],
      "metadata": {
        "id": "XCay2HRlMcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i36QqXZSMmF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input your path where you want to save the trained model**"
      ],
      "metadata": {
        "id": "qkOes-P_SXCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happy_gen = HappyTextToText(\"T5\", \"t5-base\")\n",
        "path_to_save = \"model/\"\n",
        "happy_gen.save(path_to_save)"
      ],
      "metadata": {
        "id": "BQVO3vttOtdD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}