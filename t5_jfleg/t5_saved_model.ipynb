{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vh0rvTht7q0",
        "outputId": "96cc753e-e1ff-4de8-c8ee-54ad8095863d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Path where the multiGed datasets are stored**"
      ],
      "metadata": {
        "id": "ze8TTqkgf1vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/My Drive/ENLP_Project/My_try/en_fce_train.tsv'\n",
        "dev_file = '/content/drive/My Drive/ENLP_Project/My_try/en_fce_dev.tsv'\n",
        "test_file = '/content/drive/My Drive/ENLP_Project/My_try/en_fce_dev.tsv'"
      ],
      "metadata": {
        "id": "LGUZSjhtt-i6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happytransformer\n",
        "\n",
        "from happytransformer import HappyTextToText\n"
      ],
      "metadata": {
        "id": "I5_bBO3hs0t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import TTSettings\n",
        "\n",
        "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=100)\n"
      ],
      "metadata": {
        "id": "heaPiZMzuJRQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember to enter the path of the folder in which your model is saved**"
      ],
      "metadata": {
        "id": "hAkW9oeoANu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/My Drive/ENLP_Project/My_try/saved_model/model/\"\n",
        "happy_tt = HappyTextToText(\"T5\", \"t5-base\",load_path=model_path)"
      ],
      "metadata": {
        "id": "XKwoyBVAUo24"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"grammar: This sentences, has bads grammer and spelling!\"\n",
        "res = happy_tt.generate_text(example, args=beam_settings)\n",
        "print(res.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk94hV53uLmT",
        "outputId": "8d7d88c9-8f1a-4e5b-9b28-561409cc100a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This sentence has bad grammer and spelling!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_2 = \"grammar: I enjoys, writtings articles ons AI.\"\n",
        "\n",
        "result_2 = happy_tt.generate_text(example_2, args=beam_settings)\n",
        "print(result_2.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtoyuN0WcfWy",
        "outputId": "946fe110-2d65-45c8-a093-7200157820c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy writing articles on AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(dev_file, 'r') as f:\n",
        "\n",
        "  sentence_list = []\n",
        "  sent = []\n",
        "  i_tags = []\n",
        "  c_tags = []\n",
        "  all_tags = []\n",
        "  n = 0\n",
        "  correct_preds = 0\n",
        "  total_preds = 0\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  for i,l in enumerate(f):\n",
        "    if l.split() != []:\n",
        "      if l.split()[0] != \".\":\n",
        "        sent.append(l.split()[0])\n",
        "\n",
        "        all_tags.append(l.split()[1])\n",
        "\n",
        "        if l.split()[1] == 'i':\n",
        "          # print(\"-->>\",l.split()[1],l.split()[0])\n",
        "          # print(\"n\",n)\n",
        "          i_tags.append(n)\n",
        "\n",
        "        if l.split()[1] == 'c':\n",
        "          c_tags.append(n) \n",
        "\n",
        "        n += 1\n",
        "\n",
        "        # tags.append(l.split()[1])\n",
        "        # print(l.split()[0])\n",
        "      else:\n",
        "        # print(\"entire sentence\",\" \".join(sent))\n",
        "        sentence_list.append(\" \".join(sent))\n",
        "        wrong_sent = \"grammar: \" + \" \".join(sent)\n",
        "        print(\"---Given sentence---: \", wrong_sent)\n",
        "        # print(\"wrong_sent\",wrong_sent,wrong_sent.split() ,len(wrong_sent.split())-1)\n",
        "        # if len(i_tags) > 0:\n",
        "        #   print(\"wrong_word\",sent[i_tags[0]])\n",
        "        # print(\"----tags----\",tags)\n",
        "        # print(\"--all_tags--\",all_tags)\n",
        "        result_1 = happy_tt.generate_text(wrong_sent, args=beam_settings)\n",
        "        # print(\"i_tags\",i_tags)\n",
        "        # print(result_1.text, result_1.text.split(), len(result_1.text.split())-1)\n",
        "\n",
        "        print(\"---corrected sentence---: \",result_1.text)\n",
        "\n",
        "\n",
        "        try:\n",
        "          if len(i_tags) > 0:\n",
        "            # print(\"----->>>>>>>>\",i_tags)\n",
        "            # print(\"right_word\",result_1.text.split()[i_tags[0]])\n",
        "\n",
        "            # try:\n",
        "            pred_len = len(result_1.text.split())\n",
        "            # if len(sent) in (pred_len,pred_len+1,pred_len-1):\n",
        "            for t in i_tags:\n",
        "              # print(\"---incorrect_word---\",sent[t])\n",
        "              # print(\"---correct_word---\",result_1.text.split()[t])\n",
        "\n",
        "      \n",
        "\n",
        "              if sent[t] != result_1.text.split()[t]:\n",
        "                tp += 1\n",
        "\n",
        "                # correct_preds += 1\n",
        "\n",
        "              elif sent[t] == result_1.text.split()[t]:\n",
        "          #       # print(\"incorrect\",sent[t],result_1.text.split()[t])\n",
        "          #       # because the prediction is supposed to be correct, but false...spmethig like that\n",
        "                fp += 1\n",
        "                \n",
        "\n",
        "              total_preds += 1\n",
        "            i_tags = []\n",
        "\n",
        "\n",
        "            for c in c_tags:\n",
        "        #     # print(\"---corrections---\",sent[c],result_1.text.split()[c])\n",
        "\n",
        "              if sent[c] == result_1.text.split()[c]:\n",
        "                tp += 1\n",
        "              else:\n",
        "        #         # print(\"incorrect++++++\",sent[c],result_1.text.split()[c])\n",
        "                fn += 1\n",
        "\n",
        "              total_preds += 1\n",
        "            c_tags = []\n",
        "            # # except:\n",
        "            #   print(\"\")\n",
        "\n",
        "          # i_tags = []\n",
        "          # c_tags = []\n",
        "\n",
        "        except:\n",
        "          print(\"\")\n",
        "            \n",
        "\n",
        "        i_tags = []\n",
        "        c_tags = []\n",
        "\n",
        "        print(\"\\n\")\n",
        "        sent = []\n",
        "        tags = []\n",
        "        all_tags = []\n",
        "        n = 0\n",
        "\n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  print(\"precision: \",precision*100)\n",
        "  print(\"recall: \", recall*100)\n",
        "  print(\"accuracy: \",(tp/total_preds)*100)\n",
        "  print(\"f_0.5: \", (1.25 * precision * recall) / (0.25 * precision + recall)*100)\n",
        "\n"
      ],
      "metadata": {
        "id": "_bjS0JoZYXJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}